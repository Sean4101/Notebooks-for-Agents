{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from PO_grid_world import PO_GridWorld\n",
    "from notebook_env_wrapper import NotebookEnvWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_po = PO_GridWorld(partially_observable=True)\n",
    "env_notebook = NotebookEnvWrapper(PO_GridWorld(partially_observable=True), notebook_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrows = [\"↑\", \"↓\", \"←\", \"→\"]\n",
    "\n",
    "def print_policy(model):\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            obs = i*6 + j\n",
    "            pred = model.predict(obs, deterministic=True)[0]\n",
    "            print(arrows[pred], end=\" \")\n",
    "        print()\n",
    "\n",
    "def print_policy_po(po_model):\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            obs = (i//3)*2 + (j//3)\n",
    "            pred = po_model.predict(obs, deterministic=True)[0]\n",
    "            print(arrows[pred], end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↑ ↑ ↑ ↑ ↑ ↑ \n",
      "↑ ↑ ↑ ↑ ↑ ↑ \n",
      "↑ ↑ ↑ ↑ ↑ ↑ \n",
      "→ → → ↑ ↑ ↑ \n",
      "→ → → ↑ ↑ ↑ \n",
      "→ → → ↑ ↑ ↑ \n"
     ]
    }
   ],
   "source": [
    "model_po = PPO.load(\"models_cmp/ppo_gridworld_po_2\")\n",
    "print_policy_po(model_po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\Notebook\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO Model 0: 0.73 +/- 0.68, mean episode length: 79.10 +/- 65.56\n",
      "PO Model 1: 0.75 +/- 0.66, mean episode length: 77.41 +/- 63.00\n",
      "PO Model 2: 0.78 +/- 0.62, mean episode length: 75.93 +/- 59.46\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "mean_episode_lengths_po = []\n",
    "std_episode_lengths_po = []\n",
    "mean_rewards_po = []\n",
    "std_rewards_po = []\n",
    "\n",
    "for i in range(n):\n",
    "    model = PPO.load(f\"models_cmp/ppo_gridworld_po_{i}\")\n",
    "    rewards, lengths = evaluate_policy(model, env_po, n_eval_episodes=1000, return_episode_rewards=True)\n",
    "\n",
    "    mean_episode_length = np.mean(lengths)\n",
    "    std_episode_length = np.std(lengths)\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "\n",
    "    mean_episode_lengths_po.append(mean_episode_length)\n",
    "    std_episode_lengths_po.append(std_episode_length)\n",
    "    mean_rewards_po.append(mean_reward)\n",
    "    std_rewards_po.append(std_reward)\n",
    "    print(f\"PO Model {i}: {mean_reward:.2f} +/- {std_reward:.2f}, mean episode length: {mean_episode_length:.2f} +/- {std_episode_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Model 0: 0.87 +/- 0.49, mean episode length: 29.10 +/- 34.52\n",
      "Notebook Model 1: 0.86 +/- 0.52, mean episode length: 31.15 +/- 40.49\n",
      "Notebook Model 2: 0.84 +/- 0.54, mean episode length: 30.25 +/- 37.03\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "mean_episode_lengths_notebook = []\n",
    "std_episode_lengths_notebook = []\n",
    "mean_rewards_notebook = []\n",
    "std_rewards_notebook = []\n",
    "\n",
    "for i in range(n):\n",
    "    model = PPO.load(f\"models_cmp/ppo_gridworld_notebook_{i}\")\n",
    "    rewards, lengths = evaluate_policy(model, env_notebook, n_eval_episodes=1000, return_episode_rewards=True)\n",
    "    \n",
    "    mean_episode_length = np.mean(lengths)\n",
    "    std_episode_length = np.std(lengths)\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "\n",
    "    mean_episode_lengths_notebook.append(mean_episode_length)\n",
    "    std_episode_lengths_notebook.append(std_episode_length)\n",
    "    mean_rewards_notebook.append(mean_reward)\n",
    "    std_rewards_notebook.append(std_reward)\n",
    "    print(f\"Notebook Model {i}: {mean_reward:.2f} +/- {std_reward:.2f}, mean episode length: {mean_episode_length:.2f} +/- {std_episode_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO models\n",
      "Mean episode length: 77.48 +/- 62.68\n",
      "Mean reward: 0.76 +/- 0.65\n",
      "Notebook models\n",
      "Mean episode length: 30.17 +/- 37.35\n",
      "Mean reward: 0.86 +/- 0.51\n"
     ]
    }
   ],
   "source": [
    "print(\"PO models\")\n",
    "print(f\"Mean episode length: {np.mean(mean_episode_lengths_po):.2f} +/- {np.mean(std_episode_lengths_po):.2f}\")\n",
    "print(f\"Mean reward: {np.mean(mean_rewards_po):.2f} +/- {np.mean(std_rewards_po):.2f}\")\n",
    "print(\"Notebook models\")\n",
    "print(f\"Mean episode length: {np.mean(mean_episode_lengths_notebook):.2f} +/- {np.mean(std_episode_lengths_notebook):.2f}\")\n",
    "print(f\"Mean reward: {np.mean(mean_rewards_notebook):.2f} +/- {np.mean(std_rewards_notebook):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(-1.0), np.float64(1.0), np.float64(1.0)], [np.int64(14), np.int64(31), np.int64(14), np.int64(20), np.int64(15), np.int64(87), np.int64(75), np.int64(14), np.int64(143), np.int64(11)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\Notebook\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = PPO.load(f\"models_cmp/ppo_gridworld_notebook_0\")\n",
    "rewards = evaluate_policy(model, env_notebook, n_eval_episodes=10, return_episode_rewards=True)\n",
    "\n",
    "print(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
